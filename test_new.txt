Perfect ‚Äî this is exactly the right moment to lock a high-quality, low-redo ‚Äúsystem-style‚Äù prompt for your internal tool.
Below is a clean, institutional-grade master prompt designed specifically for:
Annual reports
Primary impact assessment
Social Opportunities Fund
Analyst / PM consumption
Single-document, single-session constraint
You can paste this as the very first prompt after uploading each report, then append your 8 questions at the end.
üîπ MASTER PROMPT ‚Äî PRIMARY SOCIAL IMPACT ASSESSMENT
(Copy‚Äìpaste and reuse across all reports)
You are acting as an Impact Analyst supporting a Social Opportunities Fund.
The uploaded document is a single company annual report.
Your task is to perform a primary social impact assessment strictly based on information disclosed in this report.
Scope & Data Rules
Use only the uploaded annual report.
Do not infer, estimate, benchmark, or assume information that is not explicitly disclosed.
If data is missing, unclear, or qualitative only, clearly state ‚ÄúNot disclosed in the annual report.‚Äù
Do not use external knowledge, peer comparisons, or prior-year assumptions unless explicitly stated in the report.
Analytical Lens (VERY IMPORTANT)
Assess the company through a Social Opportunities Fund lens, focusing on:
Access to essential products or services
Positive social outcomes for underserved or vulnerable populations
Employee wellbeing, safety, inclusion, and fair treatment
Human capital development and workforce resilience
Responsible customer practices and product impact
This is a primary assessment, not a final scoring or investment recommendation.
Answering Instructions
For each question:
Answer only that question (do not combine responses)
Reference page numbers and/or section titles wherever possible
Prioritize quantitative disclosures; supplement with qualitative detail if relevant
Keep responses concise, factual, and analyst-ready
If multiple disclosures relate to the same topic, summarize them clearly
Output Format (MANDATORY)
For each question, use the following structure:
Answer:
Clear, direct response to the question
Evidence from the Annual Report:
Bullet points with page number(s) and/or section name(s)
Disclosure Gaps (if any):
Explicitly state missing, partial, or unclear information
Do not add opinions, conclusions, or scores unless explicitly asked.
Answer the following questions based only on the uploaded annual report:
üîπ After this, paste your 8 questions like this
Copy code

1. [Question 1]
2. [Question 2]
3. [Question 3]
...
8. [Question 8]
Why this prompt works well in internal tools
‚úÖ Prevents hallucinations
‚úÖ Forces page-level traceability (PM-friendly)
‚úÖ Handles missing data cleanly (very important for impact)
‚úÖ Consistent outputs across all companies
‚úÖ Minimizes need to restart sessions
Optional (only if allowed by your tool)
If your internal tool supports tables, add this line before the questions:
If information is best presented in a table (e.g., workforce breakdowns, safety metrics), use a simple table and cite pages.
If you want, next I can:
Refine this for your exact 8 questions (even tighter)
Convert it into a one-click reusable template
Add a ‚ÄúPM summary paragraph‚Äù version without increasing reruns
Stress-test it against a sample annual report question
Just tell me üëçFlat hierarchies
Multi-level nested hierarchies
Normalizes text across hierarchy levels
MLOps principle:
‚úî Deterministic preprocessing
‚úî Reproducible inference inputs
3Ô∏è‚É£ Embedding Generation (Pluggable Backend)
Decision Point
If Local Mode:
CPU-based sentence transformers
Deterministic, cost-efficient
If Cloud Mode:
Vertex AI text embeddings
Managed, scalable, versioned
Secure access via service accounts
Runtime behavior
Batch embedding generation
Backend selected dynamically
Automatic fallback on failure
MLOps principle:
‚úî Model abstraction
‚úî Resilience without redeployments
4Ô∏è‚É£ Similarity Computation & Ranking
Application computes similarity using multiple metrics:
Cosine, Euclidean, Manhattan
Dot Product, Minkowski, Chebyshev
Jaccard similarity
Metrics run in parallel
Results ranked per metric
Why this matters
Avoids single-metric bias
Enables explainability
Supports metric comparison
MLOps principle:
‚úî Transparent evaluation
‚úî Explainable inference logic
5Ô∏è‚É£ Result Assembly & UI Output
Ranked matches displayed in UI
Each hierarchy level shown explicitly
Metric scores exposed (not hidden)
Results downloadable for offline analysis
MLOps principle:
‚úî Traceable outputs
‚úî Human-interpretable inference
6Ô∏è‚É£ Stateless Execution & Operational Guarantees
No data persisted between requests
Each request processed independently
Safe for repeated and concurrent usage
Always-on service managed via systemd
MLOps principle:
‚úî Stateless inference
‚úî Production-safe execution model
7Ô∏è‚É£ MLOps Maturity Mapping (Inline)
‚ùå Level 0: Manual inference logic
‚úÖ Level 1: Automated, repeatable inference flow embedded in an application
‚ö†Ô∏è Level 2: No retraining or feedback loops (not required for this use case)
Key Takeaway (Flow-Based)
The application implements a fully automated inference flow ‚Äî from user input to ranked, explainable outputs ‚Äî demonstrating Level-1 MLOps maturity without requiring training pipelines.
Why this works extremely well
Reads like how the app actually runs
Easy to narrate in < 2 minutes
Naturally explains why training is absent
Shows MLOps thinking inside application design
If you want next, I can:
Convert this into a 1-slide diagram layout
Add icons per step (input ‚Üí model ‚Üí output)
Create a speaker narration script
Align this visually with Google MLOps Level markers
This flow version is exactly what reviewers expect when they ask
‚ÄúOkay, but how does it actually work?‚Äù
