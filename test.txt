Slide 1 — Use Case Framing & MLOps Maturity Target
Why this use case exists and what it demonstrates
Context
Implemented as part of the MLOps Working Group as a reference production use case
Focused on demonstrating end-to-end MLOps practices, not just model performance
Use case: semantic taxonomy classification using embeddings
Why this use case
Common enterprise problem (taxonomy mapping across ESG, products, research)
Embedding-based inference → ideal for demonstrating inference MLOps
Stateless, repeatable, and production-friendly
MLOps Objective
Move from:
Ad-hoc scripts and notebooks (Level 0)
To a repeatable, deployable ML service (Level 1)
Establish a baseline that can evolve toward Level 2
Google MLOps Alignment
Primary target: Level 1 (ML pipeline automation – inference)
Select elements approaching Level 2 (CI/CD, reliability patterns)
Key message:
This project is designed to demonstrate incremental MLOps maturity, not over-engineering.
Slide 2 — ML System Design & Reproducibility Foundations
Core requirement for Level 1 MLOps
ML System Decomposition
Clear separation of concerns:
Input ingestion (CSV / JSON / YAML)
Embedding generation
Similarity computation
Ranking & output generation
Reproducibility Practices
Version-controlled codebase
Pinned Python dependencies
Isolated Python virtual environment
Deterministic local inference option
Model Abstraction
Embedding models are:
Configurable
Replaceable
Not hard-coded into application logic
Google MLOps Mapping
❌ Level 0: Manual, non-reproducible scripts
✅ Level 1: Reproducible ML systems
⚠️ Level 2: No automated promotion workflows yet
MLOps principle:
Reproducibility is the first non-negotiable step toward production ML.
Slide 3 — Model Lifecycle & Inference Automation
Operational ML without training pipelines
Embedding Lifecycle
Local Embeddings
CPU-based sentence transformers
Cost-efficient, deterministic
Ideal for development, demos, and controlled environments
Cloud Embeddings (Vertex AI)
Managed, scalable, versioned
Enterprise-grade inference
Secure access via service accounts
Inference Automation
Batch embedding generation
Runtime model selection
Automatic fallback between models
Stateless execution (no persisted state)
Evaluation Strategy
7 similarity metrics:
Cosine, Euclidean, Manhattan
Dot Product, Minkowski, Chebyshev
Jaccard similarity
Enables robustness, explainability, and metric comparison
Google MLOps Mapping
❌ Level 0: Manual inference logic
✅ Level 1: Automated inference pipeline
⚠️ Level 2: No automated retraining triggers (not applicable here)
Key insight:
MLOps applies equally to inference-only systems, not just training pipelines.
Slide 4 — Deployment, Reliability & Operations
Production readiness with pragmatic MLOps
Current Deployment
Deployed on Google Cloud Compute Engine
Python 3.11 + Streamlit application
Managed as a systemd service
Always-on VM → no cold starts
Reliability Practices
Auto-restart on failure
Service health managed at OS level
Predictable runtime behavior
Observability
Application-level logs
System-level logs (journalctl)
Manual operational controls:
Restart service
Check status
Tail logs
Intentional Design Choice
No Docker / Kubernetes
Architecture chosen to:
Minimize operational overhead
Match predictable workload profile
Google MLOps Mapping
❌ Level 0: Ad-hoc execution
✅ Level 1: Stable, repeatable deployment
⚠️ Level 2: Partial observability (no metrics dashboards yet)
Maturity signal:
Strong MLOps outcomes can be achieved without complex orchestration.
Slide 5 — Security, Governance, Cost & Access
Often overlooked, but critical for enterprise MLOps
Security
Service-account–based access to Vertex AI
Least-privilege IAM
Firewall-controlled SSH access
No hard-coded credentials
Governance
Deployed in MapleQuad GCP project
Region-aligned with org policies
Stateless processing (no data persistence)
Cost Control
Predictable infrastructure cost (< $10/month)
Optional cloud inference usage
Clear cost vs scalability trade-offs
Access & Transparency
Public demo access:
Application URL (for functional testing)
Engineering access:
SSH via gcloud CLI to inspect code, logs, and service
Enables:
Peer review
Knowledge sharing
Hands-on learning within WG
MLOps principle:
A production ML system must be secure, auditable, and operable, not just accurate.
Slide 6 — MLOps Maturity Assessment, Roadmap & Experiments
Where we are today and what comes next
Current Maturity Assessment (Google MLOps)
Area
Status
Reproducibility
✅ Level 1
Inference Automation
✅ Level 1
Deployment Reliability
✅ Level 1
Security & IAM
✅ Level 1
Monitoring & Metrics
⚠️ Partial
CI/CD
❌ Not yet
Governance Automation
⚠️ Basic
Overall Classification
Solid Level 1 MLOps implementation
Strong foundation for Level 2 evolution
Near-Term Roadmap
Deploy and validate within MapleQuad GCP project
Run controlled testing before wider internal exposure
Collect usage and operational feedback
Future MLOps Experiments
Cloud Run deployment
Compare VM-based always-on vs serverless
Evaluate cold starts, cost, and scaling
CI/CD pipelines
Metrics & latency dashboards
Automated validation checks
Optional containerization when scale demands it
Final Takeaway
This use case establishes a pragmatic, reviewable, and reusable Level 1 MLOps baseline, with a clear and intentional path toward Level 2.
